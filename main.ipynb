{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'c:\\Users\\beer\\AppData\\Local\\Programs\\Python\\Python311\\python.exe' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'c:/Users/beer/AppData/Local/Programs/Python/Python311/python.exe -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "#QUESTION 1 Data Ingestion\n",
    "\n",
    "# In this case I'll write the solutions as if the data is STREAMED rather than batched, will comment on the README for possible swaps \n",
    "\n",
    "# INFRAESTRUCTURE REQUIREMENTS AND DETAILS\n",
    "\n",
    "#Bigquery dataset and table setup\n",
    "\n",
    "from google.cloud import bigquery\n",
    "\n",
    "client = bigquery.Client()\n",
    "dataset_id = \"project-test-nw.RAW_TRAVEL\"\n",
    "dataset = bigquery.Dataset(dataset_id)\n",
    "dataset.location = \"us-central1\"\n",
    "dataset = client.create_dataset(dataset, timeout=30)  # Make an API request.\n",
    "print(f\"Created dataset { dataset_id }\")\n",
    "\n",
    "#Create the input tables with clustering and partitioning (as requirements)\n",
    "\n",
    "table_id = \"project-test-nw.RAW_TRAVEL.RAW_INPUT_TRAVEL\"\n",
    "\n",
    "schema = [\n",
    "    bigquery.SchemaField(\"region\", \"STRING\"),\n",
    "    bigquery.SchemaField(\"origin_coord\", \"GEOGRAPHY\"),\n",
    "    bigquery.SchemaField(\"destination_coord\", \"GEOGRAPHY\"),\n",
    "    bigquery.SchemaField(\"datetime\", \"TIMESTAMP\"),\n",
    "    bigquery.SchemaField(\"datasource\", \"STRING\")\n",
    "]\n",
    "\n",
    "table = bigquery.Table(table_id, schema=schema)\n",
    "table.clustering_fields = [\"datetime\", \"origin_coord\",\"destination_coord\"]\n",
    "table.time_partitioning = bigquery.TimePartitioning(\n",
    "    type_=bigquery.TimePartitioningType.DAY,\n",
    "    field=\"datetime\",  # name of column to use for partitioning\n",
    "    expiration_ms=7776000000,\n",
    ")  # 90 days worth of partitioning\n",
    "table = client.create_table(table)  # Make an API request.\n",
    "print(\n",
    "    \"Created clustered and partitioned table {}\".format(\n",
    "        table_id\n",
    "    )\n",
    ")\n",
    "\n",
    "#subscription creation, so there IS a pubsub to bigquery subscription without dataflow \n",
    "#that i will opt to exploit because i'm lazy, and also because it can handle tons of data\n",
    "# Source: https://cloud.google.com/pubsub/docs/subscriber\n",
    "\n",
    "from google.cloud import pubsub_v1\n",
    "from google.cloud.pubsub_v1.types import DeadLetterPolicy\n",
    "\n",
    "project_id = \"project-test-nw\"\n",
    "topic_id = \"input-topic\"\n",
    "dead_letter_topic_id=\"deadletter-input-topic\"\n",
    "subscription_id = \"data-raw-travel\"\n",
    "bigquery_table_id = f\"{project_id}.RAW_TRAVEL.RAW_INPUT_TRAVEL\"\n",
    "\n",
    "dead_letter_topic_path=f\"projects/{project_id}/topics/{dead_letter_topic_id}\"\n",
    "max_delivery_attempts=5\n",
    "\n",
    "publisher = pubsub_v1.PublisherClient()\n",
    "subscriber = pubsub_v1.SubscriberClient()\n",
    "topic_path = publisher.topic_path(project_id, topic_id)\n",
    "subscription_path = subscriber.subscription_path(project_id, subscription_id)\n",
    "\n",
    "bigquery_config = pubsub_v1.types.BigQueryConfig(\n",
    "    table=bigquery_table_id, write_metadata=True\n",
    ")\n",
    "\n",
    "dead_letter_topic_path = publisher.topic_path(project_id, dead_letter_topic_id)\n",
    "\n",
    "dead_letter_policy = DeadLetterPolicy(\n",
    "    dead_letter_topic=dead_letter_topic_path,\n",
    "    max_delivery_attempts=max_delivery_attempts,\n",
    ")\n",
    "\n",
    "# Wrap the subscriber in a 'with' block to automatically call close() to\n",
    "# close the underlying gRPC channel when done.\n",
    "with subscriber:\n",
    "    subscription = subscriber.create_subscription(\n",
    "        request={\n",
    "            \"name\": subscription_path,\n",
    "            \"topic\": topic_path,\n",
    "            \"bigquery_config\": bigquery_config,\n",
    "            \"dead_letter_policy\": dead_letter_policy\n",
    "        }\n",
    "    )\n",
    "\n",
    "print(f\"BigQuery subscription created: {subscription}.\")\n",
    "print(f\"Table for subscription is: {bigquery_table_id}\")\n",
    "\n",
    "\n",
    "#Create the deadletter subscription to the assigned table (GCP asumes the structure of the pub sub message as the bigquery table schema)\n",
    "from google.cloud import pubsub_v1\n",
    "\n",
    "bigquery_dead_table_id = f\"{project_id}.RAW_TRAVEL.RAW_INPUT_TRAVEL_DEADLETTER\"\n",
    "\n",
    "publisher = pubsub_v1.PublisherClient()\n",
    "subscriber = pubsub_v1.SubscriberClient()\n",
    "topic_path = publisher.topic_path(project_id, dead_letter_topic_id)\n",
    "subscription_path = subscriber.subscription_path(project_id, subscription_id)\n",
    "\n",
    "bigquery_config = pubsub_v1.types.BigQueryConfig(\n",
    "    table=bigquery_table_id, write_metadata=True\n",
    ")\n",
    "\n",
    "# Wrap the subscriber in a 'with' block to automatically call close() to\n",
    "# close the underlying gRPC channel when done.\n",
    "with subscriber:\n",
    "    subscription = subscriber.create_subscription(\n",
    "        request={\n",
    "            \"name\": subscription_path,\n",
    "            \"topic\": topic_path,\n",
    "            \"bigquery_config\": bigquery_config,\n",
    "        }\n",
    "    )\n",
    "\n",
    "print(f\"BigQuery subscription created: {subscription}.\")\n",
    "print(f\"Table for subscription is: {bigquery_table_id}\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Service weekly (cloud FUNction)\n",
    "\n",
    "#deploy this script to a HTTP cloud function so it can act as a service\n",
    "\n",
    "import functions_framework\n",
    "from google.cloud import bigquery\n",
    "\n",
    "from google.oauth2 import service_account\n",
    "\n",
    "@functions_framework.http\n",
    "def cors_enabled_function(request):\n",
    "    try:\n",
    "        \n",
    "        credentials = service_account.Credentials.from_service_account_file('path/to/file.json')\n",
    "\n",
    "        project_id = \"project-test-nw\"\n",
    "        bigquery_table_id = f\"{project_id}.RAW_TRAVEL.RAW_INPUT_TRAVEL\"\n",
    "\n",
    "        client = bigquery.Client(credentials= credentials,project=project_id)\n",
    "        \n",
    "\n",
    "        #Example request\n",
    "        #request={\n",
    "        #    \"version\": \"1.0.0\",\n",
    "        #    \"min_x\": 535,\n",
    "        #    \"min_y\": 340,\n",
    "        #    \"max_x\": 964,\n",
    "        #    \"max_y\": 684, \n",
    "        #    \"region\": \"Poopland\"\n",
    "        #}\n",
    "\n",
    "\n",
    "        query=''' \n",
    "        WITH \n",
    "            box as ( \n",
    "                SELECT STRUCT(\n",
    "                {min_lat} AS min_lat,\n",
    "                {max_lat} AS max_lat,\n",
    "                {min_lon} AS min_lon,\n",
    "                {max_lon} AS max_lon\n",
    "            ))\n",
    "\n",
    "            travels AS (\n",
    "            SELECT t.orgin_coord,t.destination_coord,t.datetime \n",
    "            FROM `{table}` AS t \n",
    "            WHERE t.region=\"{region}\" \n",
    "            AND t.datetime >= TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL 8 WEEK)\n",
    "            AND ST_WITHIN(t.orgin_coord, box)\n",
    "            AND ST_WITHIN(t.destination_coord, box)\n",
    "            ), \n",
    "            weeks AS (\n",
    "            SELECT\n",
    "            DATE_TRUNC(datetime, WEEK) AS week_start_date,\n",
    "            COUNT(1) AS row_count\n",
    "            FROM travels\n",
    "            GROUP BY week_start_date\n",
    "            ORDER BY week_start_date;   \n",
    "            )\n",
    "            \n",
    "        SELECT AVG(row_count) FROM weeks;\n",
    "            \n",
    "        \n",
    "\n",
    "        '''.format(table=bigquery_table_id,region=request[\"region\"],min_lat=request[\"min_x\"],max_lat=request[\"max_x\"],min_lon=request[\"min_y\"],max_lon=request[\"max_y\"])\n",
    "        \n",
    "        query_job = client.query(query)\n",
    "        results = query_job.result()\n",
    "        return (results, 200)\n",
    "    except Exception as e:\n",
    "        return (\"Something went wrong!\",500)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get status of ingestion data flow (cloud FUNction)\n",
    "#In this case I opted for a service that compares good/bad entries in the last hour and then serve a percentage of success, if there are no bad messages in the last hour, the service will return 1 (meaning 100% ok messages in the last hour)\n",
    "\n",
    "import functions_framework\n",
    "from google.cloud import bigquery\n",
    "from google.oauth2 import service_account\n",
    "\n",
    "@functions_framework.http\n",
    "def cors_enabled_function(request):\n",
    "    try:\n",
    "        \n",
    "        credentials = service_account.Credentials.from_service_account_file('path/to/file.json')\n",
    "\n",
    "        project_id = \"project-test-nw\"\n",
    "        bigquery_table_id = f\"{project_id}.RAW_TRAVEL.RAW_INPUT_TRAVEL\"\n",
    "        bigquery_table_deadletter_id = f\"{project_id}.RAW_TRAVEL.RAW_INPUT_TRAVEL\"\n",
    "\n",
    "        client = bigquery.Client(credentials= credentials,project=project_id)\n",
    "\n",
    "        query=''' SELECT DIV(count(ti.datetime),SUM(count(ti.datetime),count(td.datetime)) )  \n",
    "        FROM `{table_input}` ti, `{table_deadletter}` td \n",
    "        WHERE ti.datetime >= TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL 1 HOUR) \n",
    "        AND td.datetime >= TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL 1 HOUR)\n",
    "\n",
    "          '''.format(table_input=bigquery_table_id,table_deadletter=bigquery_table_deadletter_id)\n",
    "        query_job = client.query(query)\n",
    "        results = query_job.result()\n",
    "        return (results, 200)\n",
    "    except Exception as e:\n",
    "        return (\"Something went wrong!\",500)\n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
